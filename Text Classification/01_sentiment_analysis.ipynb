{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6a9940b",
   "metadata": {},
   "source": [
    "![DLI Header](images/DLI_Header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbd94ad",
   "metadata": {},
   "source": [
    "# Text Classification #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ae13d1",
   "metadata": {},
   "source": [
    "## Sentimental Analysis ##\n",
    "\n",
    "In this notebook, you will learn to fine-tune a pre-trained model. Specifically, we will use a model for sentiment analysis. \n",
    "\n",
    "**Sentiment Analysis** is the task of detecting the sentiment in text. We model this problem as a simple form of a text classification problem. For example `Gollum's performance is incredible!` has a positive sentiment while `It's neither as romantic nor as thrilling as it should be.` has a negative sentiment. In such an analysis, we need to look at sentences, and we only have two classes: \"positive\" and \"negative\". Each sentence in the training set must be labeled as one or the other. Sentiment analysis is widely used by businesses to identify customer sentiment toward products, brands, or services in online conversations and feedback.\n",
    "\n",
    "**Table of Contents**<br>\n",
    "This notebook covers the below sections: \n",
    "* Dataset\n",
    "    * Download and Preprocess data\n",
    "    * Labeling Data (OPTIONAL)\n",
    "* Use Pre-Trained Model\n",
    "    * Download Model\n",
    "    * Make Predictions\n",
    "* Fine-Tune a Pre-Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293b6159",
   "metadata": {},
   "source": [
    "## Dataset ##\n",
    "\n",
    "In this notebook, we're going to use The [Stanford Sentiment Treebank (SST-2)](https://nlp.stanford.edu/sentiment/index.html) corpus for sentiment analysis. The data contains a collection of sentences with binary labels for positive and negative. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15477684",
   "metadata": {},
   "source": [
    "For text classification, NeMo requires the data to be in a specific format. Data needs to be in TAB separated files (.tsv) with two columns of sentence and label. Each line of the data file contains text sequences, where words are separated with spaces and label separated with [TAB], i.e.: `[WORD] [SPACE] [WORD] [SPACE] [WORD] [TAB] [LABEL]`\n",
    "\n",
    "For example: \n",
    "* \n",
    "```\n",
    "hide new secretions from the parental units[TAB]0\n",
    "that loves its characters and communicates something rather beautiful about human nature[TAB]1\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d8e705",
   "metadata": {},
   "source": [
    "### Download and Preprocess Data ###\n",
    "\n",
    "We have prepared the SST-2 dataset for you. It should contain three files of train.tsv, dev.tsv, and test.tsv which can be used for `training`, `validation`, and `test` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "498b372d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import wget\n",
    "\n",
    "# set data path\n",
    "DATA_DIR='data'\n",
    "DATA_DIR=os.path.join(DATA_DIR, 'SST-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80fa0313",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 34772\n",
      "-rw-r--r-- 1 root root   597450 Nov  2  2023 cached_dev_nemo_format.tsv_BertTokenizer_256_30522_-1_0_False.pkl\n",
      "-rw-r--r-- 1 root root 26988143 Nov  2  2023 cached_train_nemo_format.tsv_BertTokenizer_256_30522_-1_0_True.pkl\n",
      "-rw-r--r-- 1 root root    94931 Nov  2  2023 dev.tsv\n",
      "-rw-r--r-- 1 root root    94916 Nov  2  2023 dev_nemo_format.tsv\n",
      "drwxr-xr-x 2 root root     4096 Sep 17 05:21 original\n",
      "-rw-r--r-- 1 root root   197335 Nov  2  2023 test.tsv\n",
      "-rw-r--r-- 1 root root  3806081 Nov  2  2023 train.tsv\n",
      "-rw-r--r-- 1 root root  3806066 Nov  2  2023 train_nemo_format.tsv\n"
     ]
    }
   ],
   "source": [
    "# check that data folder should contain train.tsv, dev.tsv, test.tsv\n",
    "!ls -l {DATA_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf7ac5c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "sentence\tlabel\n",
      "hide new secretions from the parental units \t0\n",
      "contains no wit , only labored gags \t0\n",
      "that loves its characters and communicates something rather beautiful about human nature \t1\n",
      "remains utterly satisfied to remain the same throughout \t0\n",
      "Dev:\n",
      "sentence\tlabel\n",
      "it 's a charming and often affecting journey . \t1\n",
      "unflinchingly bleak and desperate \t0\n",
      "allows us to hope that nolan is poised to embark a major career as a commercial yet inventive filmmaker . \t1\n",
      "the acting , costumes , music , cinematography and sound are all astounding given the production 's austere locales . \t1\n",
      "Test:\n",
      "index\tsentence\n",
      "0\tuneasy mishmash of styles and genres .\n",
      "1\tthis film 's relationship to actual tension is the same as what christmas-tree flocking in a spray can is to actual snow : a poor -- if durable -- imitation .\n",
      "2\tby the end of no such thing the audience , like beatrice , has a watchful affection for the monster .\n",
      "3\tdirector rob marshall went out gunning to make a great one .\n"
     ]
    }
   ],
   "source": [
    "# preview data \n",
    "print('Train:')\n",
    "!head -n 5 {DATA_DIR}/train.tsv\n",
    "\n",
    "print('Dev:')\n",
    "!head -n 5 {DATA_DIR}/dev.tsv\n",
    "\n",
    "print('Test:')\n",
    "!head -n 5 {DATA_DIR}/test.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572c8f4c",
   "metadata": {},
   "source": [
    "The format of `train.tsv` and `dev.tsv` is close to NeMo's format except to have an extra header line at the beginning of the files. We would remove these extra lines. But `test.tsv` has different format and labels are missing for this part of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d319078f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed 1d {DATA_DIR}/train.tsv > {DATA_DIR}/train_nemo_format.tsv\n",
    "!sed 1d {DATA_DIR}/dev.tsv > {DATA_DIR}/dev_nemo_format.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be07bd3b",
   "metadata": {},
   "source": [
    "## Fine-Tune a Pre-Trained Model ##\n",
    "\n",
    "A text classification model is typically comprised of a pre-trained [BERT](https://arxiv.org/pdf/1810.04805.pdf) model followed by a text classification layer. For training, we can use a configuration file to define the model. The configuration (config) file consists of several important sections, including: \n",
    "* **model**: All arguments that are related to the Model - language model, token classifier, optimizer and schedulers, datasets and any other related information\n",
    "* **trainer**: Any argument to be passed to PyTorch Lightning\n",
    "\n",
    "_Note:_ NeMo provides a template for creating the configuration file, which is recommended as a starting point, but you can create your own as long as it follows the required format. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591f8ac9",
   "metadata": {},
   "source": [
    "### Configuration File ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "879167e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define config path\n",
    "MODEL_CONFIG=\"text_classification_config.yaml\"\n",
    "WORK_DIR='WORK_DIR'\n",
    "os.makedirs(WORK_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b662c221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config file already exists\n"
     ]
    }
   ],
   "source": [
    "# download the model's configuration file \n",
    "BRANCH='main'\n",
    "config_dir = WORK_DIR + '/configs/'\n",
    "os.makedirs(config_dir, exist_ok=True)\n",
    "if not os.path.exists(config_dir + MODEL_CONFIG):\n",
    "    print('Downloading config file...')\n",
    "    wget.download(f'https://raw.githubusercontent.com/NVIDIA/NeMo/{BRANCH}/examples/nlp/text_classification/conf/' + MODEL_CONFIG, config_dir)\n",
    "else:\n",
    "    print ('config file already exists')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cf15c6",
   "metadata": {},
   "source": [
    "The config file for text classification, `text_classification_config.yaml`, specifies model, training, and experiment management details, such as file locations, pre-trained models, and hyperparameters. The YAML config file we downloaded provides default values for most of the parameters, but there are a few items that must be specified for this experiment.\n",
    "\n",
    "Each YAML section is a bit easier to view using the `omegaconf` package, which allows you to access and manipulate the configuration keys using a \"dot\" notation. We'll take a look at the details of each section using the `OmegaConf` tool. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76c8b03b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer:\n",
      "  devices: 1\n",
      "  num_nodes: 1\n",
      "  max_epochs: 100\n",
      "  max_steps: -1\n",
      "  accumulate_grad_batches: 1\n",
      "  gradient_clip_val: 0.0\n",
      "  precision: 32\n",
      "  accelerator: gpu\n",
      "  log_every_n_steps: 1\n",
      "  val_check_interval: 1.0\n",
      "  num_sanity_val_steps: 0\n",
      "  enable_checkpointing: false\n",
      "  logger: false\n",
      "model:\n",
      "  nemo_path: text_classification_model.nemo\n",
      "  tokenizer:\n",
      "    tokenizer_name: ${model.language_model.pretrained_model_name}\n",
      "    vocab_file: null\n",
      "    tokenizer_model: null\n",
      "    special_tokens: null\n",
      "  language_model:\n",
      "    pretrained_model_name: bert-base-uncased\n",
      "    lm_checkpoint: null\n",
      "    config_file: null\n",
      "    config: null\n",
      "  classifier_head:\n",
      "    num_output_layers: 2\n",
      "    fc_dropout: 0.1\n",
      "  class_labels:\n",
      "    class_labels_file: null\n",
      "  dataset:\n",
      "    num_classes: ???\n",
      "    do_lower_case: false\n",
      "    max_seq_length: 256\n",
      "    class_balancing: null\n",
      "    use_cache: false\n",
      "  train_ds:\n",
      "    file_path: null\n",
      "    batch_size: 64\n",
      "    shuffle: true\n",
      "    num_samples: -1\n",
      "    num_workers: 3\n",
      "    drop_last: false\n",
      "    pin_memory: false\n",
      "  validation_ds:\n",
      "    file_path: null\n",
      "    batch_size: 64\n",
      "    shuffle: false\n",
      "    num_samples: -1\n",
      "    num_workers: 3\n",
      "    drop_last: false\n",
      "    pin_memory: false\n",
      "  test_ds:\n",
      "    file_path: null\n",
      "    batch_size: 64\n",
      "    shuffle: false\n",
      "    num_samples: -1\n",
      "    num_workers: 3\n",
      "    drop_last: false\n",
      "    pin_memory: false\n",
      "  optim:\n",
      "    name: adam\n",
      "    lr: 2.0e-05\n",
      "    betas:\n",
      "    - 0.9\n",
      "    - 0.999\n",
      "    weight_decay: 0.01\n",
      "    sched:\n",
      "      name: WarmupAnnealing\n",
      "      warmup_steps: null\n",
      "      warmup_ratio: 0.1\n",
      "      last_epoch: -1\n",
      "      monitor: val_loss\n",
      "      reduce_on_plateau: false\n",
      "  infer_samples:\n",
      "  - by the end of no such thing the audience , like beatrice , has a watchful affection\n",
      "    for the monster .\n",
      "  - director rob marshall went out gunning to make a great one .\n",
      "  - uneasy mishmash of styles and genres .\n",
      "exp_manager:\n",
      "  exp_dir: null\n",
      "  name: TextClassification\n",
      "  create_tensorboard_logger: true\n",
      "  create_checkpoint_callback: true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "CONFIG_DIR = \"/dli/task/WORK_DIR/configs\"\n",
    "CONFIG_FILE = \"text_classification_config.yaml\"\n",
    "\n",
    "config=OmegaConf.load(CONFIG_DIR + \"/\" + CONFIG_FILE)\n",
    "\n",
    "# print the entire configuration file\n",
    "print(OmegaConf.to_yaml(config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7305ca3d",
   "metadata": {},
   "source": [
    "Notice that some config lines, including `model.dataset.data_dir`, have `???` in place of paths, this means that values for these fields are required to be specified by the user. Details about the model arguments can be found in the [documentation](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/text_classification.html#training-the-text-classification-model). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8bd082",
   "metadata": {},
   "source": [
    "We first need to set the num_classes in the config file which specifies the number of classes in the dataset. For SST-2, we have just two classes (0-positive and 1-negative). So we set the num_classes to 2. The model supports more than 2 classes too.\n",
    "\n",
    "We need to specify and set the `model.train_ds.file_name`, `model.validation_ds.file_name`, and `model.test_ds.file_name` in the config file to the paths of the train, validation, and test files if they exist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3624338b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cached_dev_nemo_format.tsv_BertTokenizer_256_30522_-1_0_False.pkl\n",
      "cached_train_nemo_format.tsv_BertTokenizer_256_30522_-1_0_True.pkl\n",
      "dev.tsv\n",
      "dev_nemo_format.tsv\n",
      "original\n",
      "test.tsv\n",
      "train.tsv\n",
      "train_nemo_format.tsv\n"
     ]
    }
   ],
   "source": [
    "!ls $DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30fa7489",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nemo_path: text_classification_model.nemo\n",
      "tokenizer:\n",
      "  tokenizer_name: ${model.language_model.pretrained_model_name}\n",
      "  vocab_file: null\n",
      "  tokenizer_model: null\n",
      "  special_tokens: null\n",
      "language_model:\n",
      "  pretrained_model_name: bert-base-uncased\n",
      "  lm_checkpoint: null\n",
      "  config_file: null\n",
      "  config: null\n",
      "classifier_head:\n",
      "  num_output_layers: 2\n",
      "  fc_dropout: 0.1\n",
      "class_labels:\n",
      "  class_labels_file: null\n",
      "dataset:\n",
      "  num_classes: 2\n",
      "  do_lower_case: false\n",
      "  max_seq_length: 256\n",
      "  class_balancing: null\n",
      "  use_cache: false\n",
      "train_ds:\n",
      "  file_path: data/SST-2/train_nemo_format.tsv\n",
      "  batch_size: 64\n",
      "  shuffle: true\n",
      "  num_samples: -1\n",
      "  num_workers: 3\n",
      "  drop_last: false\n",
      "  pin_memory: false\n",
      "validation_ds:\n",
      "  file_path: data/SST-2/dev_nemo_format.tsv\n",
      "  batch_size: 64\n",
      "  shuffle: false\n",
      "  num_samples: -1\n",
      "  num_workers: 3\n",
      "  drop_last: false\n",
      "  pin_memory: false\n",
      "test_ds:\n",
      "  file_path: null\n",
      "  batch_size: 64\n",
      "  shuffle: false\n",
      "  num_samples: -1\n",
      "  num_workers: 3\n",
      "  drop_last: false\n",
      "  pin_memory: false\n",
      "optim:\n",
      "  name: adam\n",
      "  lr: 2.0e-05\n",
      "  betas:\n",
      "  - 0.9\n",
      "  - 0.999\n",
      "  weight_decay: 0.01\n",
      "  sched:\n",
      "    name: WarmupAnnealing\n",
      "    warmup_steps: null\n",
      "    warmup_ratio: 0.1\n",
      "    last_epoch: -1\n",
      "    monitor: val_loss\n",
      "    reduce_on_plateau: false\n",
      "infer_samples:\n",
      "- by the end of no such thing the audience , like beatrice , has a watchful affection\n",
      "  for the monster .\n",
      "- director rob marshall went out gunning to make a great one .\n",
      "- uneasy mishmash of styles and genres .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set num_classes to 2\n",
    "config.model.dataset.num_classes=2\n",
    "\n",
    "# set file paths\n",
    "config.model.train_ds.file_path = os.path.join(DATA_DIR, 'train_nemo_format.tsv')\n",
    "config.model.validation_ds.file_path = os.path.join(DATA_DIR, 'dev_nemo_format.tsv')\n",
    "\n",
    "# You may change other params like batch size or the number of samples to be considered (-1 means all the samples)\n",
    "\n",
    "# print the model section\n",
    "print(OmegaConf.to_yaml(config.model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "524c6321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "devices: 1\n",
      "num_nodes: 1\n",
      "max_epochs: 100\n",
      "max_steps: -1\n",
      "accumulate_grad_batches: 1\n",
      "gradient_clip_val: 0.0\n",
      "precision: 32\n",
      "accelerator: gpu\n",
      "log_every_n_steps: 1\n",
      "val_check_interval: 1.0\n",
      "num_sanity_val_steps: 0\n",
      "enable_checkpointing: false\n",
      "logger: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(OmegaConf.to_yaml(config.trainer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05f696c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "devices: 1\n",
      "num_nodes: 1\n",
      "max_epochs: 1\n",
      "max_steps: -1\n",
      "accumulate_grad_batches: 1\n",
      "gradient_clip_val: 0.0\n",
      "precision: 32\n",
      "accelerator: gpu\n",
      "log_every_n_steps: 1\n",
      "val_check_interval: 1.0\n",
      "num_sanity_val_steps: 0\n",
      "enable_checkpointing: false\n",
      "logger: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lets modify some trainer configs\n",
    "\n",
    "# setup max number of steps to reduce training time for demonstration purposes of this tutorial\n",
    "# Training stops when max_step or max_epochs is reached (earliest)\n",
    "config.trainer.max_epochs = 1\n",
    "\n",
    "# print the trainer section\n",
    "print(OmegaConf.to_yaml(config.trainer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8006e1ac",
   "metadata": {},
   "source": [
    "Note: `OmegaConf.to_yaml()` is used to create a proper format for printing the config. Once the `text_classification_config.yaml` file has been loaded into memory, changing the configuration file will require the config variable to be re-defined."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19615adb",
   "metadata": {},
   "source": [
    "Now, we are ready to initialize our model. During the model initialization call, the dataset and data loaders will be prepared for training and evaluation. Also, the pretrained BERT model will be downloaded, which can take up to a few minutes depending on the size of the chosen BERT model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f9c3bb",
   "metadata": {},
   "source": [
    "### Download Pre-Trained Model ###\n",
    "\n",
    "Before initializing the model, we might want to modify some of the model configs. For example, we might want to modify the pretrained BERT model to another model. The default model is `bert-base-uncased`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42f779b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE! Installing ujson may make loading annotations faster.\n",
      "bert-base-uncased\n",
      "bert-large-uncased\n",
      "bert-base-cased\n",
      "bert-large-cased\n",
      "bert-base-multilingual-uncased\n",
      "bert-base-multilingual-cased\n",
      "bert-base-chinese\n",
      "bert-base-german-cased\n",
      "bert-large-uncased-whole-word-masking\n",
      "bert-large-cased-whole-word-masking\n",
      "bert-large-uncased-whole-word-masking-finetuned-squad\n",
      "bert-large-cased-whole-word-masking-finetuned-squad\n",
      "bert-base-cased-finetuned-mrpc\n",
      "bert-base-german-dbmdz-cased\n",
      "bert-base-german-dbmdz-uncased\n",
      "cl-tohoku/bert-base-japanese\n",
      "cl-tohoku/bert-base-japanese-whole-word-masking\n",
      "cl-tohoku/bert-base-japanese-char\n",
      "cl-tohoku/bert-base-japanese-char-whole-word-masking\n",
      "TurkuNLP/bert-base-finnish-cased-v1\n",
      "TurkuNLP/bert-base-finnish-uncased-v1\n",
      "wietsedv/bert-base-dutch-cased\n",
      "distilbert-base-uncased\n",
      "distilbert-base-uncased-distilled-squad\n",
      "distilbert-base-cased\n",
      "distilbert-base-cased-distilled-squad\n",
      "distilbert-base-german-cased\n",
      "distilbert-base-multilingual-cased\n",
      "distilbert-base-uncased-finetuned-sst-2-english\n",
      "camembert-base\n",
      "Musixmatch/umberto-commoncrawl-cased-v1\n",
      "Musixmatch/umberto-wikipedia-uncased-v1\n",
      "roberta-base\n",
      "roberta-large\n",
      "roberta-large-mnli\n",
      "distilroberta-base\n",
      "roberta-base-openai-detector\n",
      "roberta-large-openai-detector\n",
      "albert-base-v1\n",
      "albert-large-v1\n",
      "albert-xlarge-v1\n",
      "albert-xxlarge-v1\n",
      "albert-base-v2\n",
      "albert-large-v2\n",
      "albert-xlarge-v2\n",
      "albert-xxlarge-v2\n",
      "gpt2\n",
      "gpt2-medium\n",
      "gpt2-large\n",
      "gpt2-xl\n",
      "distilgpt2\n"
     ]
    }
   ],
   "source": [
    "from nemo.collections import nlp as nemo_nlp\n",
    "\n",
    "# complete list of supported BERT-like models\n",
    "for model in nemo_nlp.modules.get_pretrained_lm_models_list(): \n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e9124d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the BERT-like model, you want to use\n",
    "# set the `model.language_modelpretrained_model_name' parameter in the config to the model you want to use\n",
    "config.model.language_model.pretrained_model_name = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d0ba07",
   "metadata": {},
   "source": [
    "Now, we are ready to initialize our model. During the model initialization call, the dataset and data loaders will also be prepared for the training and validation.\n",
    "\n",
    "Also, the pretrained BERT model will be automatically downloaded. Note it can take up to a few minutes depending on the size of the chosen BERT model for the first time you create the model. If your dataset is large, it also may take some time to read and process all the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93ea4573",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-09-17 05:35:08 tokenizer_utils:130] Getting HuggingFace AutoTokenizer with pretrained_model_name: bert-base-uncased, vocab_file: None, merges_files: None, special_tokens_dict: {}, and use_fast: False\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd5d6dd6b65c4779a1a73ae7190a7acc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8c0c8bb284948dcb45bb773e266e032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04d6e69c3cab4c199fef7282f69abe3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using eos_token, but it is not set yet.\n",
      "Using bos_token, but it is not set yet.\n",
      "[NeMo W 2025-09-17 05:35:08 modelPT:244] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-09-17 05:35:08 text_classification_dataset:120] Read 67349 examples from data/SST-2/train_nemo_format.tsv.\n",
      "[NeMo I 2025-09-17 05:35:08 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2025-09-17 05:35:08 text_classification_dataset:239] example 0: ['love', 'for', 'the', 'movies', 'of', 'the', '1960s']\n",
      "[NeMo I 2025-09-17 05:35:08 text_classification_dataset:240] subtokens: [CLS] love for the movies of the 1960s [SEP]\n",
      "[NeMo I 2025-09-17 05:35:08 text_classification_dataset:241] input_ids: 101 2293 2005 1996 5691 1997 1996 4120 102\n",
      "[NeMo I 2025-09-17 05:35:08 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2025-09-17 05:35:08 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2025-09-17 05:35:08 text_classification_dataset:244] label: 1\n",
      "[NeMo I 2025-09-17 05:35:08 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2025-09-17 05:35:08 text_classification_dataset:239] example 1: ['inert', 'sci-fi', 'action', 'thriller']\n",
      "[NeMo I 2025-09-17 05:35:08 text_classification_dataset:240] subtokens: [CLS] in ##ert sci - fi action thriller [SEP]\n",
      "[NeMo I 2025-09-17 05:35:08 text_classification_dataset:241] input_ids: 101 1999 8743 16596 1011 10882 2895 10874 102\n",
      "[NeMo I 2025-09-17 05:35:08 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2025-09-17 05:35:08 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2025-09-17 05:35:08 text_classification_dataset:244] label: 0\n",
      "[NeMo I 2025-09-17 05:35:48 data_preprocessing:404] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2025-09-17 05:35:48 data_preprocessing:406] Min: 3 |                  Max: 66 |                  Mean: 13.319262349849293 |                  Median: 10.0\n",
      "[NeMo I 2025-09-17 05:35:48 data_preprocessing:412] 75 percentile: 18.00\n",
      "[NeMo I 2025-09-17 05:35:48 data_preprocessing:413] 99 percentile: 44.00\n",
      "[NeMo I 2025-09-17 05:35:49 text_classification_dataset:120] Read 872 examples from data/SST-2/dev_nemo_format.tsv.\n",
      "[NeMo I 2025-09-17 05:35:49 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2025-09-17 05:35:49 text_classification_dataset:239] example 0: ['it', \"'s\", 'a', 'charming', 'and', 'often', 'affecting', 'journey', '.']\n",
      "[NeMo I 2025-09-17 05:35:49 text_classification_dataset:240] subtokens: [CLS] it ' s a charming and often affecting journey . [SEP]\n",
      "[NeMo I 2025-09-17 05:35:49 text_classification_dataset:241] input_ids: 101 2009 1005 1055 1037 11951 1998 2411 12473 4990 1012 102\n",
      "[NeMo I 2025-09-17 05:35:49 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2025-09-17 05:35:49 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2025-09-17 05:35:49 text_classification_dataset:244] label: 1\n",
      "[NeMo I 2025-09-17 05:35:49 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2025-09-17 05:35:49 text_classification_dataset:239] example 1: ['unflinchingly', 'bleak', 'and', 'desperate']\n",
      "[NeMo I 2025-09-17 05:35:49 text_classification_dataset:240] subtokens: [CLS] un ##fl ##in ##ching ##ly bleak and desperate [SEP]\n",
      "[NeMo I 2025-09-17 05:35:49 text_classification_dataset:241] input_ids: 101 4895 10258 2378 8450 2135 21657 1998 7143 102\n",
      "[NeMo I 2025-09-17 05:35:49 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2025-09-17 05:35:49 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2025-09-17 05:35:49 text_classification_dataset:244] label: 0\n",
      "[NeMo I 2025-09-17 05:35:50 data_preprocessing:404] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2025-09-17 05:35:50 data_preprocessing:406] Min: 4 |                  Max: 55 |                  Mean: 25.163990825688074 |                  Median: 24.5\n",
      "[NeMo I 2025-09-17 05:35:50 data_preprocessing:412] 75 percentile: 32.00\n",
      "[NeMo I 2025-09-17 05:35:50 data_preprocessing:413] 99 percentile: 51.29\n",
      "[NeMo I 2025-09-17 05:35:50 text_classification_model:193] Dataloader config or file_path for the test is missing, so no data loader for test is created!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b22504af3db41eea0d8f469875f5ab0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nemo.collections.nlp.models import TextClassificationModel\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "trainer=pl.Trainer(**config.trainer)\n",
    "text_classification_model=TextClassificationModel(cfg=config.model, trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb2efa5",
   "metadata": {},
   "source": [
    "### Model Training ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64bde8bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-09-17 05:36:14 modelPT:721] Optimizer config = Adam (\n",
      "    Parameter Group 0\n",
      "        amsgrad: False\n",
      "        betas: [0.9, 0.999]\n",
      "        capturable: False\n",
      "        differentiable: False\n",
      "        eps: 1e-08\n",
      "        foreach: None\n",
      "        fused: None\n",
      "        lr: 2e-05\n",
      "        maximize: False\n",
      "        weight_decay: 0.01\n",
      "    )\n",
      "[NeMo I 2025-09-17 05:36:14 lr_scheduler:910] Scheduler \"<nemo.core.optim.lr_scheduler.WarmupAnnealing object at 0x7ff2f3d4ace0>\" \n",
      "    will be used during training (effective maximum steps = 1053) - \n",
      "    Parameters : \n",
      "    (warmup_steps: null\n",
      "    warmup_ratio: 0.1\n",
      "    last_epoch: -1\n",
      "    max_steps: 1053\n",
      "    )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name                  | Type                 | Params\n",
      "---------------------------------------------------------------\n",
      "0 | loss                  | CrossEntropyLoss     | 0     \n",
      "1 | bert_model            | BertEncoder          | 109 M \n",
      "2 | classifier            | SequenceClassifier   | 592 K \n",
      "3 | classification_report | ClassificationReport | 0     \n",
      "---------------------------------------------------------------\n",
      "110 M     Trainable params\n",
      "0         Non-trainable params\n",
      "110 M     Total params\n",
      "440.297   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c7fa885609145c09b55ec9f8499af8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-09-17 05:36:14 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nemo/collections/nlp/data/text_classification/text_classification_dataset.py:203: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:245.)\n",
      "      torch.LongTensor(padded_input_ids),\n",
      "    \n",
      "[NeMo W 2025-09-17 05:36:14 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nemo/collections/nlp/data/text_classification/text_classification_dataset.py:203: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:245.)\n",
      "      torch.LongTensor(padded_input_ids),\n",
      "    \n",
      "[NeMo W 2025-09-17 05:36:14 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nemo/collections/nlp/data/text_classification/text_classification_dataset.py:203: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:245.)\n",
      "      torch.LongTensor(padded_input_ids),\n",
      "    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcaddb70c42f4ed38dfa594304f1ca38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-09-17 05:38:13 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nemo/collections/nlp/data/text_classification/text_classification_dataset.py:203: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:245.)\n",
      "      torch.LongTensor(padded_input_ids),\n",
      "    \n",
      "[NeMo W 2025-09-17 05:38:13 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nemo/collections/nlp/data/text_classification/text_classification_dataset.py:203: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:245.)\n",
      "      torch.LongTensor(padded_input_ids),\n",
      "    \n",
      "[NeMo W 2025-09-17 05:38:13 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nemo/collections/nlp/data/text_classification/text_classification_dataset.py:203: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:245.)\n",
      "      torch.LongTensor(padded_input_ids),\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-09-17 05:38:13 text_classification_model:142] val_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             92.25      89.02      90.61        428\n",
      "    label_id: 1                                             89.76      92.79      91.25        444\n",
      "    -------------------\n",
      "    micro avg                                               90.94      90.94      90.94        872\n",
      "    macro avg                                               91.01      90.91      90.93        872\n",
      "    weighted avg                                            90.98      90.94      90.93        872\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "# start model training\n",
    "trainer.fit(text_classification_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b74defa",
   "metadata": {},
   "source": [
    "### Evaluate Predictions ###\n",
    "\n",
    "for inference, we can use `trainer.test()` or `model.classifytext()`. additional [documentation](https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/nlp/models/text_classification/text_classification_model.py). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09c1c7f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-09-17 05:41:52 text_classification_dataset:120] Read 872 examples from data/SST-2/dev_nemo_format.tsv.\n",
      "[NeMo I 2025-09-17 05:41:52 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2025-09-17 05:41:52 text_classification_dataset:239] example 0: ['it', \"'s\", 'a', 'charming', 'and', 'often', 'affecting', 'journey', '.']\n",
      "[NeMo I 2025-09-17 05:41:52 text_classification_dataset:240] subtokens: [CLS] it ' s a charming and often affecting journey . [SEP]\n",
      "[NeMo I 2025-09-17 05:41:52 text_classification_dataset:241] input_ids: 101 2009 1005 1055 1037 11951 1998 2411 12473 4990 1012 102\n",
      "[NeMo I 2025-09-17 05:41:52 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2025-09-17 05:41:52 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2025-09-17 05:41:52 text_classification_dataset:244] label: 1\n",
      "[NeMo I 2025-09-17 05:41:52 text_classification_dataset:238] *** Example ***\n",
      "[NeMo I 2025-09-17 05:41:52 text_classification_dataset:239] example 1: ['unflinchingly', 'bleak', 'and', 'desperate']\n",
      "[NeMo I 2025-09-17 05:41:52 text_classification_dataset:240] subtokens: [CLS] un ##fl ##in ##ching ##ly bleak and desperate [SEP]\n",
      "[NeMo I 2025-09-17 05:41:52 text_classification_dataset:241] input_ids: 101 4895 10258 2378 8450 2135 21657 1998 7143 102\n",
      "[NeMo I 2025-09-17 05:41:52 text_classification_dataset:242] segment_ids: 0 0 0 0 0 0 0 0 0 0\n",
      "[NeMo I 2025-09-17 05:41:52 text_classification_dataset:243] input_mask: 1 1 1 1 1 1 1 1 1 1\n",
      "[NeMo I 2025-09-17 05:41:52 text_classification_dataset:244] label: 0\n",
      "[NeMo I 2025-09-17 05:41:53 data_preprocessing:404] Some stats of the lengths of the sequences:\n",
      "[NeMo I 2025-09-17 05:41:53 data_preprocessing:406] Min: 4 |                  Max: 55 |                  Mean: 25.163990825688074 |                  Median: 24.5\n",
      "[NeMo I 2025-09-17 05:41:53 data_preprocessing:412] 75 percentile: 32.00\n",
      "[NeMo I 2025-09-17 05:41:53 data_preprocessing:413] 99 percentile: 51.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[NeMo W 2025-09-17 05:41:53 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      rank_zero_warn(\n",
      "    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "276a0cc39c774578a2b554221b4fb44e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-09-17 05:41:53 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nemo/collections/nlp/data/text_classification/text_classification_dataset.py:203: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:245.)\n",
      "      torch.LongTensor(padded_input_ids),\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-09-17 05:41:53 text_classification_model:142] test_report: \n",
      "    label                                                precision    recall       f1           support   \n",
      "    label_id: 0                                             92.25      89.02      90.61        428\n",
      "    label_id: 1                                             89.76      92.79      91.25        444\n",
      "    -------------------\n",
      "    micro avg                                               90.94      90.94      90.94        872\n",
      "    macro avg                                               91.01      90.91      90.93        872\n",
      "    weighted avg                                            90.98      90.94      90.93        872\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.22871354222297668,\n",
       "  'test_precision': 90.94036865234375,\n",
       "  'test_f1': 90.94036865234375,\n",
       "  'test_recall': 90.94036865234375}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_config = OmegaConf.create({'file_path': config.model.validation_ds.file_path, 'batch_size': 64, 'shuffle': False, 'num_samples': -1})\n",
    "text_classification_model.setup_test_data(test_data_config=eval_config)\n",
    "trainer.test(model=text_classification_model, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f8876e",
   "metadata": {},
   "source": [
    "### Inference ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4a9a0e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction results of some sample queries with the trained model:\n",
      "Query : by the end of no such thing the audience , like beatrice , has a watchful affection for the monster .\n",
      "Predicted label: 1\n",
      "Query : director rob marshall went out gunning to make a great one .\n",
      "Predicted label: 1\n",
      "Query : uneasy mishmash of styles and genres .\n",
      "Predicted label: 0\n"
     ]
    }
   ],
   "source": [
    "# define the list of queries for inference\n",
    "queries = ['by the end of no such thing the audience , like beatrice , has a watchful affection for the monster .', \n",
    "           'director rob marshall went out gunning to make a great one .', \n",
    "           'uneasy mishmash of styles and genres .']\n",
    "           \n",
    "# max_seq_length=512 is the maximum length BERT supports.       \n",
    "results = text_classification_model.classifytext(queries=queries, batch_size=3, max_seq_length=512)\n",
    "\n",
    "print('The prediction results of some sample queries with the trained model:')\n",
    "for query, result in zip(queries, results):\n",
    "    print(f'Query : {query}')\n",
    "    print(f'Predicted label: {result}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f78cbfad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restart the kernel\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429c1d95",
   "metadata": {},
   "source": [
    "**Well Done!** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024d266c",
   "metadata": {},
   "source": [
    "![DLI Header](images/DLI_Header.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
